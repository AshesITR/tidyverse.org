---
title: discrim 0.0.1
date: 2019-10-01
slug: discrim-0-0-1
author: Max Kuhn
categories: [package]
description: >
    discrim 0.0.1 is on CRAN. 
photo:
  url: https://unsplash.com/photos/4op9_2Bt2Eg
  author: Teo Duldulao
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", 
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px"
)
library(earth)
library(ggplot2)
theme_set(theme_bw())
```

The new package `discrim` contains `parsnip` bindings for additional classification models, including:

 * Linear discriminant analysis (LDA, simple and L2 regularized)
 * Regularized discriminant analysis (RDA, via [Friedman (1989)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Regularized+Discriminant+Analysis%22&btnG=))
 * [Flexible discriminant analysis](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Flexible+discriminant+analysis%22&btnG=) (FDA) using [MARS features](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22multivariate+adaptive+regression+splines%22&btnG=)
 * Naive Bayes models 

The package can also be used as a template for adding new models to `tidymodels` without having to directly involve `parsnip`. 

As an example, the `rsample` package contains a data set with two factors and two classes: 

```{r startup}
library(tidymodels)
library(tune)
library(discrim)
library(earth)

data("two_class_dat", package = "rsample")
two_class_dat

ggplot(two_class_dat, aes(x = A, y = B)) + 
  geom_point(aes(col = Class), alpha = .3) + 
  coord_equal() + 
  theme(legend.position = "top")
```

How would a flexible discriminant model do here? We'll split the data then fit the model:

```{r fda-1}
set.seed(115)
data_split <- initial_split(two_class_dat, prop = 2/3)
data_tr <- training(data_split)
data_te <- testing(data_split)

fda_mod <- discrim_flexible() %>% set_engine("earth")

fda_fit <- 
  fda_mod %>% 
  fit(Class ~ A + B, data = data_tr)

fda_fit 
```

Since no parameters to the model were specified, the MARS algorithm follows its own internal method for optimizing the number of features to use in the model. The results is

```{r mars}
summary(fda_fit$fit$fit)
```


The classification boundary, overlaid on the test set:

```{r grid-1}
pred_grid <- 
  expand.grid(A = seq(-.1, 4, length = 100), B = seq(0, 4, length = 100))

pred_grid <- 
  bind_cols(
    pred_grid,
    predict(fda_fit, pred_grid, type = "prob") %>% 
      select(.pred_Class1) %>% 
      setNames("fda_pred")
  )

p <-
  ggplot(data_te, aes(x = A, y = B)) + 
  geom_point(aes(col = Class), alpha = .3) + 
  coord_equal() + 
  theme(legend.position = "top")

p + 
  geom_contour(data = pred_grid, aes(z = fda_pred), breaks = .5, col = "black")
```

That seems pretty reasonable. 

These models also work with the [`tune` package](https://github.com/tidymodels/tune). To demonstrate, a regularized discriminant analysis^[Despite the name, this type of regularization is different from the more commonly used lasso (L<sub>1</sub>) or ridge (L<sub>2</sub>) regression methods. Here, the covariance matrix of the predictors is regularized in different ways as described [here](https://rdrr.io/cran/klaR/man/rda.html). ] model will be fit to the data and optimized using a simple grid search. First, we mark the parameters for tuning:

```{r rda}
rda_mod <- 
  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>% 
  set_engine("rda")
```

In order to tune the model, we require a grid of candidate values along with a resampling specification. We'll also setup a `yardstick` object to measure the area under the ROC curve for each candidate model:

```{r rda-stuff}
set.seed(20014)
folds <- vfold_cv(data_tr, repeats = 5)

candidates <- 
  rda_mod %>% 
  param_set() %>% 
  grid_max_entropy(size = 30)

roc_values <- metric_set(roc_auc)
```

Now we can tune the model:

```{r rda-tune}
rda_res <-
  tune_grid(Class ~ A + B,
            rda_mod,
            rs = folds,
            grid = candidates,
            perf = roc_values)
```

The resampling estimates for a few the top few models as:

```{r best}
auc_values <- summarize(rda_res) %>% arrange(desc(mean)) 
auc_values %>% slice(1:5)
```

Let's plot the resampling results:

```{r grid-res}
ggplot(auc_values, aes(x = frac_common_cov, y = frac_identity, size = mean)) + 
  geom_point(alpha = .5) + 
  coord_equal()
```

From this, the `frac_common_cov` parameter, which modulates between estimating a single covariance matrix or one for each class, doesn't appear to matter much for these data. The `frac_identity` clearly is better with smaller values. This indicates that the covariance matrix should _not_ be shrunken towards a simple identity matrix (i.e. uncorrelated predictors). 

Remaking the `parsnip` model object with the best parameter combination of `frac_common_cov` = `r auc_values %>% slice(1) %>% pull(frac_common_cov) %>% round(3)` and `frac_identity` = `r auc_values %>% slice(1) %>% pull(frac_identity) %>% round(3)`. The `merge` function can be used to insert these values into our original `parsnip` object:

```{r final-mod}
final_param <- 
  auc_values %>% 
  slice(1) %>% 
  select(frac_common_cov, frac_identity)

rda_mod <- 
  rda_mod %>% 
  merge(final_param) %>% 
  pull(x) %>% 
  pluck(1)

rda_mod

rda_fit <- 
  rda_mod %>% 
  fit(Class ~ A + B, data = data_tr)
```

To show the class boundary:

```{r rda-boundary}
pred_grid <- 
  bind_cols(
    pred_grid,
    predict(rda_fit, pred_grid, type = "prob") %>% 
      select(.pred_Class1) %>% 
      setNames("rda_pred")
  )

p + 
  geom_contour(data = pred_grid, aes(z = fda_pred), breaks = .5, col = "black", 
               alpha = .5, lty = 2) + 
  geom_contour(data = pred_grid, aes(z = rda_pred), breaks = .5, col = "black")
```

