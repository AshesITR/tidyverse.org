---
output: hugodown::hugo_document
slug: tidymodels-sparse-support
title: Sparse data structures in tidymodels
date: 2020-11-16
author: Julia Silge
description: >
    Sparse data is common in many domains, and now tidymodels supports using 
    sparse matrix structures throughout the fitting and tuning stages of modeling.
photo:
  url: https://unsplash.com/photos/7JX0-bfiuxQ
  author: JJ Ying
# one of: "deep-dive", "learn", "package", "programming", or "other"
categories: [learn] 
tags: [tidymodels,tune,parsnip,hardhat]

---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)
library(tidyverse)
theme_set(theme_minimal())
```

The new release of [tune](linktopost) is chock full of improvements and new features. When combined with the latest releases of [hardhat](http://hardhat.tidymodels.org/) and [parsnip](https://parsnip.tidymodels.org/), one upgrade that tidymodels users can now use in their day-to-day modeling work is **support for sparse data structures**.

## Why sparse data?

In some subject matter domains, it is common to have lots and lots of zeroes after transforming data to a representation appropriate for analysis or modeling. Text data is one such example. The `small_fine_foods` dataset of Amazon reviews of fine foods contains a column `review` that we as humans can read and understand.

```{r R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0)}
library(tidyverse)
library(tidymodels)

data("small_fine_foods")
training_data
```

Computers, on the other hand, need that `review` variable to be heavily preprocessed and transformed in order for it to be ready for most modeling. We typically need to [tokenize](https://smltar.com/tokenization.html) the text, find word frequencies, and perhaps [compute tf-idf](https://www.tidytextmining.com/tfidf.html). We can either keep the resulting data in a long, tidy tibble or we can transform it to a wide matrix, often a good fit when the next step is a modeling or machine learning algorithm.

```{r, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0)}
library(tidytext)

training_data %>%
  unnest_tokens(word, review) %>%
  count(product, word) %>%
  bind_tf_idf(word, product, n) %>%
  cast_dfm(product, word, tf_idf)
```

As is typical for text data, this document-feature matrix is extremely sparse, with many zeroes. Most documents do not contain most words. If we coerce this kind of structure to anything like a `data.frame`, we run into two challenges:

- The amount of **memory** this object requires increases dramatically.
- We lose out on the **speed** gained from any specialized model algorithms built for sparse data.

## A blueprint for sparse models

Before the most recent releases of hardhat, parsnip, and tune, there was no support for sparse data structures within tidymodels. Now, you can specify a hardhat **blueprint** for sparse data.

```{r}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")
```

The `dgCMatrix` composition is from the [Matrix](https://cran.r-project.org/package=Matrix) package, and is the most standard class for sparse numeric matrices in modeling in R. You can also specify a dense matrix composition with `composition = "matrix"`.

## Workflows and sparsity

The blueprint is used under the hood by the hardhat functions to process data. To get ready to fit our model using the sparse blueprint, we can set up our preprocessing recipe:

```{r}
library(textrecipes)

text_rec <-
  recipe(score ~ review, data = training_data) %>%
  step_tokenize(review)  %>%
  step_stopwords(review) %>%
  step_tokenfilter(review, max_tokens = 1e3) %>%
  step_tfidf(review)
```

And we set up our model as we would normally:

```{r}
lasso_spec <-
  logistic_reg(penalty = 0.02, mixture = 1) %>%
  set_engine("glmnet")
```

The regularized modeling of the glmnet package is an example of an algorithm that has specialized approaches for sparse data. If we pass in dense data with `set_engine("glmnet")`, the underlying model will take one approach, and it will use a different, faster approach especially built for sparse data if we pass in a sparse matrix. Typically, we would recommend centering and scaling predictors using `step_normalize()` before fitting a regularized model like glmnet. However, if we do this, we would no longer have all our zeroes and sparse data. Instead, we can "normalize" these text predictors using tf-idf so that they are all on the same scale.

Let's put together two workflows, one using the sparse blueprint and one using the default behavior.

```{r}
wf_sparse <- 
  workflow() %>%
  add_recipe(text_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)
  
wf_default <- 
  workflow() %>%
  add_recipe(text_rec) %>%
  add_model(lasso_spec)
```

Now let's use `fit_resamples()` to estimate how well this model fits with both options and measure performance for both.

```{r}
set.seed(123)
food_folds <- vfold_cv(training_data, v = 3)

results <- bench::mark(
  iterations = 10, check = FALSE,
  sparse = fit_resamples(wf_sparse, food_folds),  
  default = fit_resamples(wf_default, food_folds), 
)

results
```

We see on the order of a 10x speed gain by using the sparse blueprint!

```{r}
autoplot(results, type = "ridge")
```

At the same time, the model metrics are the same:

```{r}
fit_resamples(wf_sparse, food_folds) %>%
  collect_metrics()

fit_resamples(wf_default, food_folds) %>%
  collect_metrics()
```

To see a detailed text modeling example using this dataset of food reviews, _without_ sparse encodings but complete with tuning hyperparameters, check out [our article on `tidymodels.org`](https://www.tidymodels.org/learn/work/tune-text/)!

## Current limits

In tidymodels, the support for sparse data structures begins coming _out_ of a [preprocessing recipe](https://www.tmwr.org/recipes.html) and continues throughout the fitting and tuning process. There is heterogeneity in how recipes themselves handle data internally; this is why we didn't a huge decrease in memory use when comparing `wf_sparse` to `wf_default`. The [textrecipes](https://textrecipes.tidymodels.org/) package internally adopts the idea of a [tokenlist](https://textrecipes.tidymodels.org/reference/tokenlist.html), which is memory efficient for sparse data, but other recipe steps may handle data in a dense tibble structure. Keep these current limits in mind as you consider the memory requirements of your modeling projects!

